{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcgans.ipynb",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTNOzP_DJVNN",
        "colab_type": "code",
        "outputId": "7ce65439-5283-46bc-d4c7-c39f9909e95d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb2955cc750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cIyPjUPUIQx",
        "colab_type": "code",
        "outputId": "05ea5871-769e-4715-9eb9-67e0136f3edd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9WQfemRU-n6",
        "colab_type": "code",
        "outputId": "5abcb706-9ec8-47f5-be9b-44033a934ab1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/HLCV 19/Project/')\n",
        "!ls\n",
        "#!tar -xvf REMBRANDT_Annotated.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint  final-report.pdf  REMBRANDT_Annotated\t  Rembrand.tar.gz\n",
            "Code\t    Project_HLCV_19   REMBRANDT_Annotated.tar.gz  samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpECaE9c39XN",
        "colab_type": "code",
        "outputId": "a8ade256-6df7-4de8-9eb2-a852faf4d2d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        }
      },
      "source": [
        "!pip3 uninstall tensorflow\n",
        "!pip3 uninstall tensorflow-gpu\n",
        "!pip3 install tensorflow==2.0.0a0\n",
        "!pip3 install tensorflow-gpu==2.0.0a0\n",
        "\n",
        "#!pip3 install --upgrade tensorflow\n",
        "#!pip3 install --upgrade tensorflow-gpu\n",
        "#!pip3 uninstall tensorflow\n",
        "#!pip3 uninstall tensorflow-gpu\n",
        "#!pip3 install tensorflow==1.14.0 \n",
        "#!pip3 install tensorflow-gpu==1.14.0\n",
        "!python3 -c 'import tensorflow as tf; print(tf.__version__)'\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\n",
            "Collecting tensorflow==2.0.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/39/f99185d39131b8333afcfe1dcdb0629c2ffc4ecfb0e4c14ca210d620e56c/tensorflow-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (79.9MB)\n",
            "\u001b[K     |████████████████████████████████| 79.9MB 439kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (3.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (1.15.0)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow==2.0.0a0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 37.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (0.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (1.16.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (0.33.4)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow==2.0.0a0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 33.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0a0) (0.1.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0a0) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0a0) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0a0) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0a0) (2.9.0)\n",
            "Installing collected packages: tf-estimator-nightly, tb-nightly, tensorflow\n",
            "Successfully installed tb-nightly-1.14.0a20190301 tensorflow-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
            "\u001b[K     |████████████████████████████████| 332.1MB 56kB/s \n",
            "\u001b[?25hRequirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0) (1.16.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0) (3.7.1)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0) (1.14.0a20190301)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0) (0.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0) (0.1.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0) (0.33.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0a0) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0a0) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0a0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0a0) (0.15.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0a0) (2.9.0)\n",
            "Installing collected packages: tensorflow-gpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQClewBrC9Ii",
        "colab_type": "code",
        "outputId": "ee08589d-181e-48a9-aef1-acfb1390627f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!pip show tensorflow\n",
        "#!pip clean --all\n",
        "#!pip3 uninstall tensorlayer\n",
        "#!pip3 install tensorlayer\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.14.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: tensorboard, tensorflow-estimator, google-pasta, astor, wheel, numpy, grpcio, keras-applications, six, termcolor, keras-preprocessing, wrapt, gast, absl-py, protobuf\n",
            "Required-by: stable-baselines, magenta, fancyimpute\n",
            "Collecting tensorflow==2.0\n",
            "\u001b[31m  ERROR: Could not find a version that satisfies the requirement tensorflow==2.0 (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0rc0, 1.1.0rc1, 1.1.0rc2, 1.1.0, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.3.0rc0, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.4.0rc0, 1.4.0rc1, 1.4.0, 1.4.1, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.8.0rc0, 1.8.0rc1, 1.8.0, 1.9.0rc0, 1.9.0rc1, 1.9.0rc2, 1.9.0, 1.10.0rc0, 1.10.0rc1, 1.10.0, 1.10.1, 1.11.0rc0, 1.11.0rc1, 1.11.0rc2, 1.11.0, 1.12.0rc0, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.12.2, 1.12.3, 1.13.0rc0, 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for tensorflow==2.0\u001b[0m\n",
            "Collecting tensorflow-gpu==2.0\n",
            "\u001b[31m  ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.0 (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0rc2, 1.1.0, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.3.0rc0, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.4.0rc0, 1.4.0rc1, 1.4.0, 1.4.1, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.8.0rc0, 1.8.0rc1, 1.8.0, 1.9.0rc0, 1.9.0rc1, 1.9.0rc2, 1.9.0, 1.10.0rc0, 1.10.0rc1, 1.10.0, 1.10.1, 1.11.0rc0, 1.11.0rc1, 1.11.0rc2, 1.11.0, 1.12.0rc0, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.12.2, 1.12.3, 1.13.0rc0, 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for tensorflow-gpu==2.0\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PbG5tuf3PD7",
        "colab_type": "code",
        "outputId": "fff3fd65-22ea-48a3-96ac-cc2435c9130c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "from tensorlayer.layers import *\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from tensorlayer.layers import *\n",
        "def u_net(x, is_train=False, reuse=False, n_out=1):\n",
        "    _, nx, ny, nz = x.get_shape().as_list()\n",
        "    with tf.variable_scope(\"u_net\", reuse=reuse):\n",
        "        tl.layers.set_name_reuse(reuse)\n",
        "        inputs = InputLayer(x, name='inputs')\n",
        "        conv1 = Conv2d(inputs, 64, (3, 3), act=tf.nn.relu, name='conv1_1')\n",
        "        conv1 = Conv2d(conv1, 64, (3, 3), act=tf.nn.relu, name='conv1_2')\n",
        "        pool1 = MaxPool2d(conv1, (2, 2), name='pool1')\n",
        "        conv2 = Conv2d(pool1, 128, (3, 3), act=tf.nn.relu, name='conv2_1')\n",
        "        conv2 = Conv2d(conv2, 128, (3, 3), act=tf.nn.relu, name='conv2_2')\n",
        "        pool2 = MaxPool2d(conv2, (2, 2), name='pool2')\n",
        "        conv3 = Conv2d(pool2, 256, (3, 3), act=tf.nn.relu, name='conv3_1')\n",
        "        conv3 = Conv2d(conv3, 256, (3, 3), act=tf.nn.relu, name='conv3_2')\n",
        "        pool3 = MaxPool2d(conv3, (2, 2), name='pool3')\n",
        "        conv4 = Conv2d(pool3, 512, (3, 3), act=tf.nn.relu, name='conv4_1')\n",
        "        conv4 = Conv2d(conv4, 512, (3, 3), act=tf.nn.relu, name='conv4_2')\n",
        "        pool4 = MaxPool2d(conv4, (2, 2), name='pool4')\n",
        "        conv5 = Conv2d(pool4, 1024, (3, 3), act=tf.nn.relu, name='conv5_1')\n",
        "        conv5 = Conv2d(conv5, 1024, (3, 3), act=tf.nn.relu, name='conv5_2')\n",
        "\n",
        "        up4 = DeConv2d(conv5, 512, (3, 3), (nx/8, ny/8), (2, 2), name='deconv4')\n",
        "        up4 = ConcatLayer([up4, conv4], 3, name='concat4')\n",
        "        conv4 = Conv2d(up4, 512, (3, 3), act=tf.nn.relu, name='uconv4_1')\n",
        "        conv4 = Conv2d(conv4, 512, (3, 3), act=tf.nn.relu, name='uconv4_2')\n",
        "        up3 = DeConv2d(conv4, 256, (3, 3), (nx/4, ny/4), (2, 2), name='deconv3')\n",
        "        up3 = ConcatLayer([up3, conv3], 3, name='concat3')\n",
        "        conv3 = Conv2d(up3, 256, (3, 3), act=tf.nn.relu, name='uconv3_1')\n",
        "        conv3 = Conv2d(conv3, 256, (3, 3), act=tf.nn.relu, name='uconv3_2')\n",
        "        up2 = DeConv2d(conv3, 128, (3, 3), (nx/2, ny/2), (2, 2), name='deconv2')\n",
        "        up2 = ConcatLayer([up2, conv2], 3, name='concat2')\n",
        "        conv2 = Conv2d(up2, 128, (3, 3), act=tf.nn.relu,  name='uconv2_1')\n",
        "        conv2 = Conv2d(conv2, 128, (3, 3), act=tf.nn.relu, name='uconv2_2')\n",
        "        up1 = DeConv2d(conv2, 64, (3, 3), (nx/1, ny/1), (2, 2), name='deconv1')\n",
        "        up1 = ConcatLayer([up1, conv1] , 3, name='concat1')\n",
        "        conv1 = Conv2d(up1, 64, (3, 3), act=tf.nn.relu, name='uconv1_1')\n",
        "        conv1 = Conv2d(conv1, 64, (3, 3), act=tf.nn.relu, name='uconv1_2')\n",
        "        conv1 = Conv2d(conv1, n_out, (1, 1), act=tf.nn.sigmoid, name='uconv1')\n",
        "    return conv1\n",
        "\n",
        "def u_net_bn(x, is_train=False, reuse=False, batch_size=None, pad='SAME', n_out=1):\n",
        "    \"\"\"image to image translation via conditional adversarial learning\"\"\"\n",
        "    nx = int(x._shape[1])\n",
        "    ny = int(x._shape[2])\n",
        "    nz = int(x._shape[3])\n",
        "    print(\" * Input: size of image: %d %d %d\" % (nx, ny, nz))\n",
        "\n",
        "    w_init = tf.truncated_normal_initializer(stddev=0.01)\n",
        "    b_init = tf.constant_initializer(value=0.0)\n",
        "    gamma_init=tf.random_normal_initializer(1., 0.02)\n",
        "    with tf.variable_scope(\"u_net\", reuse=reuse):\n",
        "        tl.layers.set_name_reuse(reuse)\n",
        "        inputs = InputLayer(x, name='inputs')\n",
        "\n",
        "        conv1 = Conv2d(inputs, 64, (4, 4), (2, 2), act=None, padding=pad, W_init=w_init, b_init=b_init, name='conv1')\n",
        "        conv2 = Conv2d(conv1, 128, (4, 4), (2, 2), act=None, padding=pad, W_init=w_init, b_init=b_init, name='conv2')\n",
        "        conv2 = BatchNormLayer(conv2, act=lambda x: tl.act.lrelu(x, 0.2), is_train=is_train, gamma_init=gamma_init, name='bn2')\n",
        "\n",
        "        conv3 = Conv2d(conv2, 256, (4, 4), (2, 2), act=None, padding=pad, W_init=w_init, b_init=b_init, name='conv3')\n",
        "        conv3 = BatchNormLayer(conv3, act=lambda x: tl.act.lrelu(x, 0.2), is_train=is_train, gamma_init=gamma_init, name='bn3')\n",
        "\n",
        "        conv4 = Conv2d(conv3, 512, (4, 4), (2, 2), act=None, padding=pad, W_init=w_init, b_init=b_init, name='conv4')\n",
        "        conv4 = BatchNormLayer(conv4, act=lambda x: tl.act.lrelu(x, 0.2), is_train=is_train, gamma_init=gamma_init, name='bn4')\n",
        "\n",
        "        conv5 = Conv2d(conv4, 512, (4, 4), (2, 2), act=None, padding=pad, W_init=w_init, b_init=b_init, name='conv5')\n",
        "        conv5 = BatchNormLayer(conv5, act=lambda x: tl.act.lrelu(x, 0.2), is_train=is_train, gamma_init=gamma_init, name='bn5')\n",
        "\n",
        "        conv6 = Conv2d(conv5, 512, (4, 4), (2, 2), act=None, padding=pad, W_init=w_init, b_init=b_init, name='conv6')\n",
        "        conv6 = BatchNormLayer(conv6, act=lambda x: tl.act.lrelu(x, 0.2), is_train=is_train, gamma_init=gamma_init, name='bn6')\n",
        "\n",
        "        conv7 = Conv2d(conv6, 512, (4, 4), (2, 2), act=None, padding=pad, W_init=w_init, b_init=b_init, name='conv7')\n",
        "        conv7 = BatchNormLayer(conv7, act=lambda x: tl.act.lrelu(x, 0.2), is_train=is_train, gamma_init=gamma_init, name='bn7')\n",
        "\n",
        "        conv8 = Conv2d(conv7, 512, (4, 4), (2, 2), act=lambda x: tl.act.lrelu(x, 0.2), padding=pad, W_init=w_init, b_init=b_init, name='conv8')\n",
        "        print(\" * After conv: %s\" % conv8.outputs)\n",
        "        # exit()\n",
        "        # print(nx/8)\n",
        "        up7 = DeConv2d(conv8, 512, (4, 4), out_size=(2, 2), strides=(2, 2),\n",
        "                                    padding=pad, act=None, batch_size=batch_size, W_init=w_init, b_init=b_init, name='deconv7')\n",
        "        up7 = BatchNormLayer(up7, act=tf.nn.relu, is_train=is_train, gamma_init=gamma_init, name='dbn7')\n",
        "\n",
        "        # print(up6.outputs)\n",
        "        up6 = ConcatLayer([up7, conv7], concat_dim=3, name='concat6')\n",
        "        up6 = DeConv2d(up6, 1024, (4, 4), out_size=(4, 4), strides=(2, 2),\n",
        "                                    padding=pad, act=None, batch_size=batch_size, W_init=w_init, b_init=b_init, name='deconv6')\n",
        "        up6 = BatchNormLayer(up6, act=tf.nn.relu, is_train=is_train, gamma_init=gamma_init, name='dbn6')\n",
        "        # print(up6.outputs)\n",
        "        # exit()\n",
        "\n",
        "        up5 = ConcatLayer([up6, conv6], concat_dim=3, name='concat5')\n",
        "        up5 = DeConv2d(up5, 1024, (4, 4), out_size=(8, 8), strides=(2, 2),\n",
        "                                    padding=pad, act=None, batch_size=batch_size, W_init=w_init, b_init=b_init, name='deconv5')\n",
        "        up5 = BatchNormLayer(up5, act=tf.nn.relu, is_train=is_train, gamma_init=gamma_init, name='dbn5')\n",
        "        # print(up5.outputs)\n",
        "        # exit()\n",
        "\n",
        "        up4 = ConcatLayer([up5, conv5] ,concat_dim=3, name='concat4')\n",
        "        up4 = DeConv2d(up4, 1024, (4, 4), out_size=(15, 15), strides=(2, 2),\n",
        "                                    padding=pad, act=None, batch_size=batch_size, W_init=w_init, b_init=b_init, name='deconv4')\n",
        "        up4 = BatchNormLayer(up4, act=tf.nn.relu, is_train=is_train, gamma_init=gamma_init, name='dbn4')\n",
        "\n",
        "        up3 = ConcatLayer([up4, conv4] ,concat_dim=3, name='concat3')\n",
        "        up3 = DeConv2d(up3, 256, (4, 4), out_size=(30, 30), strides=(2, 2),\n",
        "                                    padding=pad, act=None, batch_size=batch_size, W_init=w_init, b_init=b_init, name='deconv3')\n",
        "        up3 = BatchNormLayer(up3, act=tf.nn.relu, is_train=is_train, gamma_init=gamma_init, name='dbn3')\n",
        "\n",
        "        up2 = ConcatLayer([up3, conv3] ,concat_dim=3, name='concat2')\n",
        "        up2 = DeConv2d(up2, 128, (4, 4), out_size=(60, 60), strides=(2, 2),\n",
        "                                    padding=pad, act=None, batch_size=batch_size, W_init=w_init, b_init=b_init, name='deconv2')\n",
        "        up2 = BatchNormLayer(up2, act=tf.nn.relu, is_train=is_train, gamma_init=gamma_init, name='dbn2')\n",
        "\n",
        "        up1 = ConcatLayer([up2, conv2] ,concat_dim=3, name='concat1')\n",
        "        up1 = DeConv2d(up1, 64, (4, 4), out_size=(120, 120), strides=(2, 2),\n",
        "                                    padding=pad, act=None, batch_size=batch_size, W_init=w_init, b_init=b_init, name='deconv1')\n",
        "        up1 = BatchNormLayer(up1, act=tf.nn.relu, is_train=is_train, gamma_init=gamma_init, name='dbn1')\n",
        "\n",
        "        up0 = ConcatLayer([up1, conv1] ,concat_dim=3, name='concat0')\n",
        "        up0 = DeConv2d(up0, 64, (4, 4), out_size=(240, 240), strides=(2, 2),\n",
        "                                    padding=pad, act=None, batch_size=batch_size, W_init=w_init, b_init=b_init, name='deconv0')\n",
        "        up0 = BatchNormLayer(up0, act=tf.nn.relu, is_train=is_train, gamma_init=gamma_init, name='dbn0')\n",
        "        # print(up0.outputs)\n",
        "        # exit()\n",
        "\n",
        "        out = Conv2d(up0, n_out, (1, 1), act=tf.nn.sigmoid, name='out')\n",
        "\n",
        "        print(\" * Output: %s\" % out.outputs)\n",
        "        # exit()\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9547c0538644>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorlayer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorlayer/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             LooseVersion(tensorflow.__version__) < LooseVersion(\"2.0.0\")):\n\u001b[1;32m     26\u001b[0m         raise RuntimeError(\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;34m\"TensorLayer does not support Tensorflow version older than 2.0.0.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;34m\"Please update Tensorflow with:\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;34m\" - `pip install --upgrade tensorflow`\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: TensorLayer does not support Tensorflow version older than 2.0.0.\nPlease update Tensorflow with:\n - `pip install --upgrade tensorflow`\n - `pip install --upgrade tensorflow-gpu`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKpebTc88Ny6",
        "colab_type": "code",
        "outputId": "c3092148-d13f-4275-c454-25922f23f81a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorlayer as tl\n",
        "import numpy as np\n",
        "import os, csv, random, gc, pickle\n",
        "import nibabel as nib\n",
        "import sys\n",
        "\n",
        "\"\"\"\n",
        "In seg file\n",
        "--------------\n",
        "Label 1: necrotic and non-enhancing tumor\n",
        "Label 2: edema \n",
        "Label 4: enhancing tumor\n",
        "Label 0: background\n",
        "\n",
        "MRI\n",
        "-------\n",
        "whole/complete tumor: 1 2 4\n",
        "core: 1 4\n",
        "enhance: 4\n",
        "\"\"\"\n",
        "###============================= SETTINGS ===================================###\n",
        "DATA_SIZE = 'all' # (small, half or all)\n",
        "\n",
        "save_dir = \"Project_HLCV_19/\"\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "HGG_data_path = \"REMBRANDT_Annotated/HGG\"\n",
        "LGG_data_path = \"REMBRANDT_Annotated/LGG\"\n",
        "survival_csv_path = \"REMBRANDT_Annotated/survival_data.csv\"\n",
        "###==========================================================================###\n",
        "\n",
        "survival_id_list = []\n",
        "survival_age_list =[]\n",
        "survival_peroid_list = []\n",
        "\n",
        "with open(survival_csv_path, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)\n",
        "    for idx, content in enumerate(reader):\n",
        "        survival_id_list.append(content[0])\n",
        "        survival_age_list.append(float(content[1]))\n",
        "        survival_peroid_list.append(float(content[2]))\n",
        "\n",
        "print(len(survival_id_list)) #163\n",
        "\n",
        "# Remove other options (half and small) for Rembrandt\n",
        "HGG_path_list = tl.files.load_folder_list(path=HGG_data_path)\n",
        "LGG_path_list = tl.files.load_folder_list(path=LGG_data_path)\n",
        "print(len(HGG_path_list), len(LGG_path_list)) #210 #75\n",
        "\n",
        "HGG_name_list = [os.path.basename(p) for p in HGG_path_list]\n",
        "LGG_name_list = [os.path.basename(p) for p in LGG_path_list]\n",
        "\n",
        "survival_id_from_HGG = []\n",
        "survival_id_from_LGG = []\n",
        "\n",
        "for i in survival_id_list:\n",
        "    if i in HGG_name_list:\n",
        "        survival_id_from_HGG.append(i)\n",
        "    elif i in LGG_name_list:\n",
        "        survival_id_from_LGG.append(i)\n",
        "    else:\n",
        "        print(i)\n",
        "\n",
        "print(len(survival_id_from_HGG), len(survival_id_from_LGG)) #62 and 40\n",
        "\n",
        "# Pick 54 out of 62 and 36 out of 40 (We will have 90 training and 12 test\n",
        "#index_HGG = list(range(0, len(survival_id_from_HGG)))\n",
        "#index_LGG = list(range(0, len(LGG_name_list)))\n",
        "index_HGG = range(len(survival_id_from_HGG))\n",
        "index_LGG = range(len(survival_id_from_LGG))\n",
        "\n",
        "# Training and testing split\n",
        "tr_index_HGG = random.sample(index_HGG, 54)\n",
        "tr_index_LGG = random.sample(index_LGG, 36)\n",
        "dev_index_HGG = [i for i in index_HGG if i not in tr_index_HGG]\n",
        "dev_index_LGG = [i for i in index_LGG if i not in tr_index_LGG]\n",
        "\n",
        "# print(tr_index_HGG)\n",
        "# print(dev_index_HGG)\n",
        "# print(tr_index_LGG)\n",
        "# print(dev_index_LGG)\n",
        "\n",
        "survival_id_dev_HGG = [survival_id_from_HGG[i] for i in dev_index_HGG]\n",
        "survival_id_tr_HGG = [survival_id_from_HGG[i] for i in tr_index_HGG]\n",
        "\n",
        "survival_id_dev_LGG = [LGG_name_list[i] for i in dev_index_LGG]\n",
        "survival_id_tr_LGG = [LGG_name_list[i] for i in tr_index_LGG]\n",
        "\n",
        "survival_age_dev = [survival_age_list[survival_id_list.index(i)] for i in survival_id_dev_HGG]\n",
        "survival_age_tr = [survival_age_list[survival_id_list.index(i)] for i in survival_id_tr_HGG]\n",
        "\n",
        "survival_period_dev = [survival_peroid_list[survival_id_list.index(i)] for i in survival_id_dev_HGG]\n",
        "survival_period_tr = [survival_peroid_list[survival_id_list.index(i)] for i in survival_id_tr_HGG]\n",
        "\n",
        "data_types = ['1', '2', '3', '4']\n",
        "data_types_mean_std_dict = {i: {'mean': 0.0, 'std': 1.0} for i in data_types}\n",
        "\n",
        "# calculate mean and std for all data types\n",
        "\n",
        "#==================== LOAD ALL IMAGES' PATH AND COMPUTE MEAN/ STD\n",
        "for i in data_types:\n",
        "    data_temp_list = []\n",
        "    for j in HGG_name_list:\n",
        "        img_path = os.path.join(HGG_data_path, j, j + '.nii.gz')\n",
        "        img = nib.load(img_path).get_data()\n",
        "        if(img.shape[0] == 256):\n",
        "            print(img.shape)\n",
        "            print(j)\n",
        "            continue\n",
        "        data_temp_list.append(img)\n",
        "        #print(img.shape)\n",
        "\n",
        "    for j in LGG_name_list:\n",
        "        img_path = os.path.join(LGG_data_path, j, j + '.nii.gz')\n",
        "        img = nib.load(img_path).get_data()\n",
        "        if(img.shape[0] == 256):\n",
        "            print(img.shape)\n",
        "            print(j)\n",
        "        data_temp_list.append(img)\n",
        "        #print(img.shape)\n",
        "\n",
        "    data_temp_list = np.concatenate(data_temp_list, axis = 2)\t\n",
        "    # data_temp_list = np.asarray(data_temp_list)\n",
        "    m = np.mean(data_temp_list)\n",
        "    s = np.std(data_temp_list)\n",
        "    data_types_mean_std_dict[i]['mean'] = m\n",
        "    data_types_mean_std_dict[i]['std'] = s\n",
        "del data_temp_list\n",
        "print(data_types_mean_std_dict)\n",
        "\n",
        "with open(save_dir + 'mean_std_dict.pickle', 'wb') as f:\n",
        "    pickle.dump(data_types_mean_std_dict, f, protocol=4)\n",
        "\n",
        "\n",
        "##==================== GET NORMALIZE IMAGES\n",
        "X_train_input = []\n",
        "X_train_target = []\n",
        "\n",
        "X_dev_input = []\n",
        "X_dev_target = []\n",
        "\n",
        "\n",
        "print(\" HGG Validation\")\n",
        "for i in survival_id_dev_HGG:\n",
        "    all_3d_data = []\n",
        "    for j in data_types:\n",
        "        # img_path = os.path.join(HGG_data_path, i, i + '_' + j + '.nii.gz')\n",
        "        img_path = os.path.join(HGG_data_path, i, i + '.nii.gz')\n",
        "        img = nib.load(img_path).get_data()\n",
        "        img = (img - data_types_mean_std_dict[j]['mean']) / data_types_mean_std_dict[j]['std']\n",
        "        img = img.astype(np.float32)\n",
        "        all_3d_data.append(img)\n",
        "\n",
        "    seg_path = os.path.join(HGG_data_path, i, i + '_seg.nii.gz')\n",
        "    seg_img = nib.load(seg_path).get_data()\n",
        "    seg_img = np.transpose(seg_img, (1, 0, 2))\n",
        "    for j in range(all_3d_data[0].shape[2]):\n",
        "        combined_array = np.stack((all_3d_data[0][:, :, j], all_3d_data[1][:, :, j], all_3d_data[2][:, :, j], all_3d_data[3][:, :, j]), axis=2)\n",
        "        combined_array = np.transpose(combined_array, (1, 0, 2))#.tolist()\n",
        "        combined_array.astype(np.float32)\n",
        "        X_dev_input.append(combined_array)\n",
        "\n",
        "        seg_2d = seg_img[:, :, j]\n",
        "        seg_2d.astype(int)\n",
        "        X_dev_target.append(seg_2d)\n",
        "    del all_3d_data\n",
        "    gc.collect()\n",
        "    print(\"finished {}\".format(i))\n",
        "\n",
        "print(\" LGG Validation\")\n",
        "for i in survival_id_dev_LGG:\n",
        "    all_3d_data = []\n",
        "    for j in data_types:\n",
        "        img_path = os.path.join(LGG_data_path, i, i + '.nii.gz')\n",
        "        img = nib.load(img_path).get_data()\n",
        "        img = (img - data_types_mean_std_dict[j]['mean']) / data_types_mean_std_dict[j]['std']\n",
        "        img = img.astype(np.float32)\n",
        "        all_3d_data.append(img)\n",
        "\n",
        "    seg_path = os.path.join(LGG_data_path, i, i + '_seg.nii.gz')\n",
        "    seg_img = nib.load(seg_path).get_data()\n",
        "    seg_img = np.transpose(seg_img, (1, 0, 2))\n",
        "    for j in range(all_3d_data[0].shape[2]):\n",
        "        combined_array = np.stack((all_3d_data[0][:, :, j], all_3d_data[1][:, :, j], all_3d_data[2][:, :, j], all_3d_data[3][:, :, j]), axis=2)\n",
        "        combined_array = np.transpose(combined_array, (1, 0, 2))#.tolist()\n",
        "        combined_array.astype(np.float32)\n",
        "        X_dev_input.append(combined_array)\n",
        "\n",
        "        seg_2d = seg_img[:, :, j]\n",
        "        seg_2d.astype(int)\n",
        "        X_dev_target.append(seg_2d)\n",
        "    del all_3d_data\n",
        "    gc.collect()\n",
        "    print(\"finished {}\".format(i))\n",
        "\n",
        "X_dev_input = np.asarray(X_dev_input, dtype=np.float32)\n",
        "X_dev_target = np.asarray(X_dev_target)#, dtype=np.float32)\n",
        "\n",
        "print(\" HGG Train\")\n",
        "for i in survival_id_tr_HGG:\n",
        "    all_3d_data = []\n",
        "    for j in data_types:\n",
        "        img_path = os.path.join(HGG_data_path, i, i + '.nii.gz')\n",
        "        img = nib.load(img_path).get_data()\n",
        "        img = (img - data_types_mean_std_dict[j]['mean']) / data_types_mean_std_dict[j]['std']\n",
        "        img = img.astype(np.float32)\n",
        "        all_3d_data.append(img)\n",
        "\n",
        "    seg_path = os.path.join(HGG_data_path, i, i + '_seg.nii.gz')\n",
        "    seg_img = nib.load(seg_path).get_data()\n",
        "    seg_img = np.transpose(seg_img, (1, 0, 2))\n",
        "    for j in range(all_3d_data[0].shape[2]):\n",
        "        combined_array = np.stack((all_3d_data[0][:, :, j], all_3d_data[1][:, :, j], all_3d_data[2][:, :, j], all_3d_data[3][:, :, j]), axis=2)\n",
        "        combined_array = np.transpose(combined_array, (1, 0, 2))#.tolist()\n",
        "        combined_array.astype(np.float32)\n",
        "        X_train_input.append(combined_array)\n",
        "\n",
        "        seg_2d = seg_img[:, :, j]\n",
        "        seg_2d.astype(int)\n",
        "        X_train_target.append(seg_2d)\n",
        "    del all_3d_data\n",
        "    print(\"finished {}\".format(i))\n",
        "\n",
        "\n",
        "print(\" LGG Train\")\n",
        "for i in survival_id_tr_LGG:\n",
        "    all_3d_data = []\n",
        "    for j in data_types:\n",
        "        img_path = os.path.join(LGG_data_path, i, i + '.nii.gz')\n",
        "        img = nib.load(img_path).get_data()\n",
        "        img = (img - data_types_mean_std_dict[j]['mean']) / data_types_mean_std_dict[j]['std']\n",
        "        img = img.astype(np.float32)\n",
        "        all_3d_data.append(img)\n",
        "\n",
        "    seg_path = os.path.join(LGG_data_path, i, i + '_seg.nii.gz')\n",
        "    seg_img = nib.load(seg_path).get_data()\n",
        "    seg_img = np.transpose(seg_img, (1, 0, 2))\n",
        "    for j in range(all_3d_data[0].shape[2]):\n",
        "        combined_array = np.stack((all_3d_data[0][:, :, j], all_3d_data[1][:, :, j], all_3d_data[2][:, :, j], all_3d_data[3][:, :, j]), axis=2)\n",
        "        combined_array = np.transpose(combined_array, (1, 0, 2))#.tolist()\n",
        "        combined_array.astype(np.float32)\n",
        "        X_train_input.append(combined_array)\n",
        "\n",
        "        seg_2d = seg_img[:, :, j]\n",
        "        seg_2d.astype(int)\n",
        "        X_train_target.append(seg_2d)\n",
        "    del all_3d_data\n",
        "    print(\"finished {}\".format(i))\n",
        "\n",
        "X_train_input = np.asarray(X_train_input, dtype=np.float32)\n",
        "X_train_target = np.asarray(X_train_target)#, dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101\n",
            "62 40\n",
            "61 40\n",
            "(256, 256, 20)\n",
            "900-00-5317\n",
            "(256, 256, 20)\n",
            "900-00-5317\n",
            "(256, 256, 20)\n",
            "900-00-5317\n",
            "(256, 256, 20)\n",
            "900-00-5317\n",
            "{'1': {'mean': 42.878201589548354, 'std': 387.6765439917516}, '2': {'mean': 42.878201589548354, 'std': 387.6765439917516}, '3': {'mean': 42.878201589548354, 'std': 387.6765439917516}, '4': {'mean': 42.878201589548354, 'std': 387.6765439917516}}\n",
            " HGG Validation\n",
            "finished 900-00-5381\n",
            "finished HF0990\n",
            "finished HF1032\n",
            "finished HF1057\n",
            "finished HF1297\n",
            "finished HF1538\n",
            "finished HF0652\n",
            " LGG Validation\n",
            "finished HF1167\n",
            "finished HF1442\n",
            "finished HF1334\n",
            "finished HF0855\n",
            " HGG Train\n",
            "finished HF1502\n",
            "finished HF1357\n",
            "finished 900-00-5342\n",
            "finished HF1628\n",
            "finished HF1587\n",
            "finished HF1191\n",
            "finished HF1613\n",
            "finished HF1150\n",
            "finished HF1136\n",
            "finished HF1122\n",
            "finished 900-00-5384\n",
            "finished HF1292\n",
            "finished HF0828\n",
            "finished HF1490\n",
            "finished HF1458\n",
            "finished 900-00-5346\n",
            "finished HF1409\n",
            "finished 900-00-5404\n",
            "finished 900-00-5385\n",
            "finished 900-00-5459\n",
            "finished HF1560\n",
            "finished 900-00-5338\n",
            "finished HF1397\n",
            "finished HF1058\n",
            "finished 900-00-5414\n",
            "finished HF1219\n",
            "finished 900-00-5339\n",
            "finished 900-00-5462\n",
            "finished 900-00-5308\n",
            "finished HF1702\n",
            "finished HF1475\n",
            "finished HF1077\n",
            "finished HF1242\n",
            "finished HF1398\n",
            "finished HF1671\n",
            "finished HF0986\n",
            "finished 900-00-5393\n",
            "finished HF1071\n",
            "finished 900-00-5316\n",
            "finished HF1139\n",
            "finished 900-00-5303\n",
            "finished HF1280\n",
            "finished 900-00-5396\n",
            "finished HF1540\n",
            "finished 900-00-5445\n",
            "finished HF1517\n",
            "finished 900-00-5332\n",
            "finished 900-00-5299\n",
            "finished HF1078\n",
            "finished HF1269\n",
            "finished HF1137\n",
            "finished 900-00-5413\n",
            "finished HF1097\n",
            "finished HF1185\n",
            " LGG Train\n",
            "finished HF1606\n",
            "finished HF1227\n",
            "finished HF1407\n",
            "finished HF1156\n",
            "finished HF1344\n",
            "finished HF1325\n",
            "finished HF1246\n",
            "finished HF0920\n",
            "finished HF1588\n",
            "finished 900-00-5341\n",
            "finished 900-00-5468\n",
            "finished HF1677\n",
            "finished HF1511\n",
            "finished 900-00-5476\n",
            "finished HF1433\n",
            "finished HF1381\n",
            "finished HF1568\n",
            "finished 900-00-5380\n",
            "finished HF1345\n",
            "finished HF1316\n",
            "finished HF0899\n",
            "finished HF0835\n",
            "finished HF1235\n",
            "finished HF0608\n",
            "finished HF1000\n",
            "finished HF0962\n",
            "finished 900-00-5412\n",
            "finished HF1553\n",
            "finished HF1489\n",
            "finished 900-00-1961\n",
            "finished HF0931\n",
            "finished HF0953\n",
            "finished HF1264\n",
            "finished HF1463\n",
            "finished 900-00-5345\n",
            "finished HF1551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICz2JvKwOCO3",
        "colab_type": "code",
        "outputId": "273e3535-ac3b-4adc-a855-133f13e153b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "#! /usr/bin/python\n",
        "# -*- coding: utf8 -*-\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "import numpy as np\n",
        "import os, time\n",
        "import sys\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "def distort_imgs(data):\n",
        "    \"\"\" data augumentation \"\"\"\n",
        "    x1, x2, x3, x4, y = data\n",
        "    # x1, x2, x3, x4, y = tl.prepro.flip_axis_multi([x1, x2, x3, x4, y],  # previous without this, hard-dice=83.7\n",
        "    #                         axis=0, is_random=True) # up down\n",
        "    x1, x2, x3, x4, y = tl.prepro.flip_axis_multi([x1, x2, x3, x4, y],\n",
        "                            axis=1, is_random=True) # left right\n",
        "    x1, x2, x3, x4, y = tl.prepro.elastic_transform_multi([x1, x2, x3, x4, y],\n",
        "                            alpha=720, sigma=24, is_random=True)\n",
        "    x1, x2, x3, x4, y = tl.prepro.rotation_multi([x1, x2, x3, x4, y], rg=20,\n",
        "                            is_random=True, fill_mode='constant') # nearest, constant\n",
        "    x1, x2, x3, x4, y = tl.prepro.shift_multi([x1, x2, x3, x4, y], wrg=0.10,\n",
        "                            hrg=0.10, is_random=True, fill_mode='constant')\n",
        "    x1, x2, x3, x4, y = tl.prepro.shear_multi([x1, x2, x3, x4, y], 0.05,\n",
        "                            is_random=True, fill_mode='constant')\n",
        "    x1, x2, x3, x4, y = tl.prepro.zoom_multi([x1, x2, x3, x4, y],\n",
        "                            zoom_range=[0.9, 1.1], is_random=True,\n",
        "                            fill_mode='constant')\n",
        "    return x1, x2, x3, x4, y\n",
        "\n",
        "def vis_imgs(X, y, path):\n",
        "    \"\"\" show one slice \"\"\"\n",
        "    if y.ndim == 2:\n",
        "        y = y[:,:,np.newaxis]\n",
        "    assert X.ndim == 3\n",
        "    tl.vis.save_images(np.asarray([X[:,:,0,np.newaxis],\n",
        "        X[:,:,1,np.newaxis], X[:,:,2,np.newaxis],\n",
        "        X[:,:,3,np.newaxis], y]), size=(1, 5),\n",
        "        image_path=path)\n",
        "\n",
        "def vis_imgs2(X, y_, y, path):\n",
        "    \"\"\" show one slice with target \"\"\"\n",
        "    if y.ndim == 2:\n",
        "        y = y[:,:,np.newaxis]\n",
        "    if y_.ndim == 2:\n",
        "        y_ = y_[:,:,np.newaxis]\n",
        "    assert X.ndim == 3\n",
        "    tl.vis.save_images(np.asarray([X[:,:,0,np.newaxis],\n",
        "        X[:,:,1,np.newaxis], X[:,:,2,np.newaxis],\n",
        "        X[:,:,3,np.newaxis], y_, y]), size=(1, 6),\n",
        "        image_path=path)\n",
        "\n",
        "def main(task='all'):\n",
        "    ## Create folder to save trained model and result images\n",
        "    save_dir = \"checkpoint\"\n",
        "    tl.files.exists_or_mkdir(save_dir)\n",
        "    tl.files.exists_or_mkdir(\"samples/{}\".format(task))\n",
        "\n",
        "    ###======================== LOAD DATA ===================================###\n",
        "    ## by importing this, you can load a training set and a validation set.\n",
        "    # you will get X_train_input, X_train_target, X_dev_input and X_dev_target\n",
        "    # there are 4 labels in targets:\n",
        "    # Label 0: background\n",
        "    # Label 1: necrotic and non-enhancing tumor\n",
        "    # Label 2: edema\n",
        "    # Label 4: enhancing tumor\n",
        "    \n",
        "    X_train = X_train_input\n",
        "    y_train = X_train_target[:,:,:,np.newaxis]\n",
        "    X_test = X_dev_input\n",
        "    y_test = X_dev_target[:,:,:,np.newaxis]\n",
        "    \n",
        "    if task == 'all':\n",
        "        y_train = (y_train > 0).astype(int)\n",
        "        y_test = (y_test > 0).astype(int)\n",
        "    elif task == 'necrotic':\n",
        "        y_train = (y_train == 1).astype(int)\n",
        "        y_test = (y_test == 1).astype(int)\n",
        "    elif task == 'edema':\n",
        "        y_train = (y_train == 2).astype(int)\n",
        "        y_test = (y_test == 2).astype(int)\n",
        "    elif task == 'enhance':\n",
        "        y_train = (y_train == 4).astype(int)\n",
        "        y_test = (y_test == 4).astype(int)\n",
        "    else:\n",
        "        exit(\"Unknow task %s\" % task)\n",
        "\n",
        "    \n",
        "    ###======================== HYPER-PARAMETERS ============================###\n",
        "    batch_size = 10\n",
        "    lr = 0.00001 \n",
        "    # lr_decay = 0.5\n",
        "    # decay_every = 100\n",
        "    beta1 = 0.9\n",
        "    n_epoch = 100\n",
        "    print_freq_step = 100\n",
        "\n",
        "    ###======================== SHOW DATA ===================================###\n",
        "    # show one slice\n",
        "    X = np.asarray(X_train[80])\n",
        "    y = np.asarray(y_train[80])\n",
        "    # print(X.shape, X.min(), X.max()) # (240, 240, 4) -0.380588 2.62761\n",
        "    # print(y.shape, y.min(), y.max()) # (240, 240, 1) 0 1\n",
        "    nw, nh, nz = X.shape\n",
        "    vis_imgs(X, y, 'samples/{}/_train_im.png'.format(task))\n",
        "    # show data augumentation results\n",
        "    for i in range(10):\n",
        "        x_flair, x_t1, x_t1ce, x_t2, label = distort_imgs([X[:,:,0,np.newaxis], X[:,:,1,np.newaxis],\n",
        "                X[:,:,2,np.newaxis], X[:,:,3,np.newaxis], y])#[:,:,np.newaxis]])\n",
        "        # print(x_flair.shape, x_t1.shape, x_t1ce.shape, x_t2.shape, label.shape) # (240, 240, 1) (240, 240, 1) (240, 240, 1) (240, 240, 1) (240, 240, 1)\n",
        "        X_dis = np.concatenate((x_flair, x_t1, x_t1ce, x_t2), axis=2)\n",
        "        # print(X_dis.shape, X_dis.min(), X_dis.max()) # (240, 240, 4) -0.380588233471 2.62376139209\n",
        "        vis_imgs(X_dis, label, 'samples/{}/_train_im_aug{}.png'.format(task, i))\n",
        "\n",
        "    with tf.device('/cpu:0'):\n",
        "        sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
        "        with tf.device('/gpu:0'): #<- remove it if you train on CPU or other GPU\n",
        "            ###======================== DEFIINE MODEL =======================###\n",
        "            ## nz is 4 as we input all Flair, T1, T1c and T2.\n",
        "            t_image = tf.placeholder('float32', [batch_size, nw, nh, nz], name='input_image')\n",
        "            ## labels are either 0 or 1\n",
        "            t_seg = tf.placeholder('float32', [batch_size, nw, nh, 1], name='target_segment')\n",
        "            ## train inference\n",
        "            net = u_net(t_image, is_train=True, reuse=False, n_out=1)\n",
        "            ## test inference\n",
        "            net_test = u_net(t_image, is_train=False, reuse=True, n_out=1)\n",
        "\n",
        "            ###======================== DEFINE LOSS =========================###\n",
        "            ## train losses\n",
        "            out_seg = net.outputs\n",
        "            dice_loss = 1 - tl.cost.dice_coe(out_seg, t_seg, axis=[0,1,2,3])#, 'jaccard', epsilon=1e-5)\n",
        "            iou_loss = tl.cost.iou_coe(out_seg, t_seg, axis=[0,1,2,3])\n",
        "            dice_hard = tl.cost.dice_hard_coe(out_seg, t_seg, axis=[0,1,2,3])\n",
        "            loss = dice_loss\n",
        "\n",
        "            ## test losses\n",
        "            test_out_seg = net_test.outputs\n",
        "            test_dice_loss = 1 - tl.cost.dice_coe(test_out_seg, t_seg, axis=[0,1,2,3])#, 'jaccard', epsilon=1e-5)\n",
        "            test_iou_loss = tl.cost.iou_coe(test_out_seg, t_seg, axis=[0,1,2,3])\n",
        "            test_dice_hard = tl.cost.dice_hard_coe(test_out_seg, t_seg, axis=[0,1,2,3])\n",
        "\n",
        "        ###======================== DEFINE TRAIN OPTS =======================###\n",
        "        t_vars = tl.layers.get_variables_with_name('u_net', True, True)\n",
        "        with tf.device('/gpu:0'):\n",
        "            with tf.variable_scope('learning_rate'):\n",
        "                lr_v = tf.Variable(lr, trainable=False)\n",
        "            train_op = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(loss, var_list=t_vars)\n",
        "\n",
        "        ###======================== LOAD MODEL ==============================###\n",
        "        tl.layers.initialize_global_variables(sess)\n",
        "        ## load existing model if possible\n",
        "        tl.files.load_and_assign_npz(sess=sess, name=save_dir+'/u_net_{}.npz'.format(task), network=net)\n",
        "\n",
        "\t\n",
        "\n",
        "    ###======================== TRAINING ================================###\n",
        "    savefile = \"log.txt\"\n",
        "    sdice_arr = []\n",
        "    hdice_arr = []\n",
        "    iou_arr = []\n",
        "    \n",
        "    sdice_arr_test = []\n",
        "    hdice_arr_test = []\n",
        "    iou_arr_test = []\n",
        "    \n",
        "    \n",
        "    f = open(savefile, 'w')\n",
        "    \n",
        "    for epoch in range(0, n_epoch+1):\n",
        "        epoch_time = time.time()\n",
        "        ## update decay learning rate at the beginning of a epoch\n",
        "        # if epoch !=0 and (epoch % decay_every == 0):\n",
        "        #     new_lr_decay = lr_decay ** (epoch // decay_every)\n",
        "        #     sess.run(tf.assign(lr_v, lr * new_lr_decay))\n",
        "        #     log = \" ** new learning rate: %f\" % (lr * new_lr_decay)\n",
        "        #     print(log)\n",
        "        # elif epoch == 0:\n",
        "        #     sess.run(tf.assign(lr_v, lr))\n",
        "        #     log = \" ** init lr: %f  decay_every_epoch: %d, lr_decay: %f\" % (lr, decay_every, lr_decay)\n",
        "        #     print(log)\n",
        "\n",
        "        total_dice, total_iou, total_dice_hard, n_batch = 0, 0, 0, 0\n",
        "        for batch in tl.iterate.minibatches(inputs=X_train, targets=y_train,\n",
        "                                    batch_size=batch_size, shuffle=True):\n",
        "            images, labels = batch\n",
        "            step_time = time.time()\n",
        "            ## data augumentation for a batch of Flair, T1, T1c, T2 images\n",
        "            # and label maps synchronously.\n",
        "            data = tl.prepro.threading_data([_ for _ in zip(images[:,:,:,0, np.newaxis],\n",
        "                    images[:,:,:,1, np.newaxis], images[:,:,:,2, np.newaxis],\n",
        "                    images[:,:,:,3, np.newaxis], labels)],\n",
        "                    fn=distort_imgs) # (10, 5, 240, 240, 1)\n",
        "            b_images = data[:,0:4,:,:,:]  # (10, 4, 240, 240, 1)\n",
        "            b_labels = data[:,4,:,:,:]\n",
        "            b_images = b_images.transpose((0,2,3,1,4))\n",
        "            b_images.shape = (batch_size, nw, nh, nz)\n",
        "\n",
        "            ## update network\n",
        "            _, _dice, _iou, _diceh, out = sess.run([train_op,\n",
        "                    dice_loss, iou_loss, dice_hard, net.outputs],\n",
        "                    {t_image: b_images, t_seg: b_labels})\n",
        "            total_dice += _dice; total_iou += _iou; total_dice_hard += _diceh\n",
        "            n_batch += 1\n",
        "\n",
        "            ## you can show the predition here:\n",
        "            # vis_imgs2(b_images[0], b_labels[0], out[0], \"samples/{}/_tmp.png\".format(task))\n",
        "            # exit()\n",
        "\n",
        "            # if _dice == 1: # DEBUG\n",
        "            #     print(\"DEBUG\")\n",
        "            #     vis_imgs2(b_images[0], b_labels[0], out[0], \"samples/{}/_debug.png\".format(task))\n",
        "\n",
        "            if n_batch % print_freq_step == 0:\n",
        "                print(\"Epoch %d step %d 1-dice: %f hard-dice: %f iou: %f took %fs (2d with distortion)\"\n",
        "                % (epoch, n_batch, _dice, _diceh, _iou, time.time()-step_time))\n",
        "\n",
        "            ## check model fail\n",
        "            if np.isnan(_dice):\n",
        "                exit(\" ** NaN loss found during training, stop training\")\n",
        "            if np.isnan(out).any():\n",
        "                exit(\" ** NaN found in output images during training, stop training\")\n",
        "\n",
        "        sdice_arr.append(total_dice/n_batch)\n",
        "        hdice_arr.append(total_dice_hard/n_batch)\n",
        "        iou_arr.append(total_iou/n_batch)\n",
        "        f.write(\" ** Epoch [%d/%d] train 1-dice: %f hard-dice: %f iou: %f took %fs (2d with distortion)\" %(epoch, n_epoch, \n",
        "        total_dice/n_batch, total_dice_hard/n_batch, total_iou/n_batch, time.time()-epoch_time))\n",
        "        print(\" ** Epoch [%d/%d] train 1-dice: %f hard-dice: %f iou: %f took %fs (2d with distortion)\" %\n",
        "                (epoch, n_epoch, total_dice/n_batch, total_dice_hard/n_batch, total_iou/n_batch, time.time()-epoch_time))\n",
        "\n",
        "        ## save a predition of training set\n",
        "        for i in range(batch_size):\n",
        "            if np.max(b_images[i]) > 0:\n",
        "                vis_imgs2(b_images[i], b_labels[i], out[i], \"samples/{}/train_{}.png\".format(task, epoch))\n",
        "                break\n",
        "            elif i == batch_size-1:\n",
        "                vis_imgs2(b_images[i], b_labels[i], out[i], \"samples/{}/train_{}.png\".format(task, epoch))\n",
        "\n",
        "        ###======================== EVALUATION ==========================###\n",
        "        total_dice, total_iou, total_dice_hard, n_batch = 0, 0, 0, 0\n",
        "        for batch in tl.iterate.minibatches(inputs=X_test, targets=y_test,\n",
        "                                        batch_size=batch_size, shuffle=True):\n",
        "            b_images, b_labels = batch\n",
        "            _dice, _iou, _diceh, out = sess.run([test_dice_loss,\n",
        "                    test_iou_loss, test_dice_hard, net_test.outputs],\n",
        "                    {t_image: b_images, t_seg: b_labels})\n",
        "            total_dice += _dice; total_iou += _iou; total_dice_hard += _diceh\n",
        "            n_batch += 1\n",
        "\t\t\n",
        "        sdice_arr_test.append(total_dice/n_batch)\n",
        "        hdice_arr_test.append(total_dice_hard/n_batch)\n",
        "        iou_arr_test.append(total_iou/n_batch)\n",
        "\t\t\n",
        "        f.write(\" **\"+\" \"*17+\"test 1-dice: %f hard-dice: %f iou: %f (2d no distortion)\" % (total_dice/n_batch, \n",
        "\ttotal_dice_hard/n_batch, total_iou/n_batch))\n",
        "        print(\" **\"+\" \"*17+\"test 1-dice: %f hard-dice: %f iou: %f (2d no distortion)\" %\n",
        "                (total_dice/n_batch, total_dice_hard/n_batch, total_iou/n_batch))\n",
        "        print(\" task: {}\".format(task))\n",
        "        ## save a predition of test set\n",
        "        for i in range(batch_size):\n",
        "            if np.max(b_images[i]) > 0:\n",
        "                vis_imgs2(b_images[i], b_labels[i], out[i], \"samples/{}/test_{}.png\".format(task, epoch))\n",
        "                break\n",
        "            elif i == batch_size-1:\n",
        "                vis_imgs2(b_images[i], b_labels[i], out[i], \"samples/{}/test_{}.png\".format(task, epoch))\n",
        "\n",
        "        ###======================== SAVE MODEL ==========================###\n",
        "        tl.files.save_npz(net.all_params, name=save_dir+'/u_net_{}.npz'.format(task), sess=sess)\n",
        "    f.close()\n",
        "\t\n",
        "    sdice_arr = np.array(sdice_arr).reshape(n_epoch+1,1)\n",
        "    hdice_arr = np.array(hdice_arr).reshape(n_epoch+1,1)\n",
        "    iou_arr = np.array(iou_arr).reshape(n_epoch+1,1)\n",
        "\n",
        "\t\n",
        "    sdice_arr_test = np.array(sdice_arr_test).reshape(n_epoch+1, 1)\n",
        "    hdice_arr_test = np.array(hdice_arr_test).reshape(n_epoch+1, 1)\n",
        "    iou_arr_test = np.array(iou_arr_test).reshape(n_epoch+1, 1)\n",
        "\n",
        "    eval_metrics = np.concatenate((sdice_arr, hdice_arr, iou_arr), axis = 1)\n",
        "    eval_metrics_test = np.concatenate((sdice_arr_test, hdice_arr_test, iou_arr_test), axis = 1)\n",
        "    \n",
        "    np.savetxt(\"eval_metrics.txt\", eval_metrics)\n",
        "    np.savetxt(\"eval_metrics_test.txt\", eval_metrics_test)\n",
        "\t\n",
        "\t\n",
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0710 18:54:51.664947 139867224057728 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "W0710 18:54:57.700238 139867224057728 tl_logging.py:123] WARNING: this method is DEPRECATED and has no effect, please remove it from your code.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-69f825f614c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-69f825f614c7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mt_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'target_segment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;31m## train inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;31m## test inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mnet_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-9547c0538644>\u001b[0m in \u001b[0;36mu_net\u001b[0;34m(x, is_train, reuse, n_out)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_name_reuse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mpool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pool1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorlayer/decorators/deprecated_alias.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mrename_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maliases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_support_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorlayer/layers/convolution/simplified_conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, prev_layer, n_filter, filter_size, strides, act, padding, data_format, dilation_rate, W_init, b_init, W_init_args, b_init_args, use_cudnn_on_gpu, name)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;31m# reuse=None,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         )\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# must put before ``new_variables``\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;31m# new_variables = tf.get_collection(TF_GRAPHKEYS_VARIABLES, scope=self.name)  #vs.name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mnew_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_collection_trainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m           \u001b[0;31m# Wrapping `call` function in autograph to allow for dynamic control\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1879\u001b[0m       \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m     \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m     \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         dtype=self.dtype)\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m       self.bias = self.add_weight(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mgetter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    385\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1494\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1237\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    560\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    512\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 864\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    865\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable u_net/conv1_1/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorlayer/layers/convolution/simplified_conv.py\", line 212, in __init__\n    self.outputs = conv2d(self.inputs)  # must put before ``new_variables``\n  File \"/usr/local/lib/python3.6/dist-packages/tensorlayer/decorators/deprecated_alias.py\", line 24, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-4-9547c0538644>\", line 13, in u_net\n    conv1 = Conv2d(inputs, 64, (3, 3), act=tf.nn.relu, name='conv1_1')\n  File \"<ipython-input-6-3331a22b486a>\", line 121, in main\n    net = u_net(t_image, is_train=True, reuse=False, n_out=1)\n  File \"<ipython-input-6-3331a22b486a>\", line 284, in <module>\n    main()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajBDg_YMJVNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Root directory for dataset\n",
        "#change this to the data location\n",
        "dataroot = \"D:\\\\SEMESTER2(ASSIGNMENTS)\\\\HLCV\\\\project\\\\dataset\\\\data\"\n",
        "\n",
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 128\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 64\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 3\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 2\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrdmYOz9JVNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dset.ImageFolder(root=dataroot,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.CenterCrop(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "# Create the dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=workers)\n",
        "\n",
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "\n",
        "# Plot some training images\n",
        "real_batch = next(iter(dataloader))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yIeMUIBJVNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pcV8hMZJVNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivkhg_-IJVNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "netG = Generator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "netG.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netG)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv9fYnCmJVNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB2LpTseJVNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the Discriminator\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "\n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "netD.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netD)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ3ZNYWuJVNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YydhS8EcJVNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Loop\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_cpu = data[0].to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, device=device)\n",
        "        # Forward pass real batch through D\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        # Generate fake image batch with G\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        # Classify all fake batch with D\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients for this batch\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Add the gradients from the all-real and all-fake batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        output = netD(fake).view(-1)http://localhost:8888/notebooks/Untitled28.ipynb?kernel_name=python3#\n",
        "        # Calculate G's loss based on this output\n",
        "        errG = criterion(output, label)\n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "\n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "        iters += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztNoYB5nJVN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}