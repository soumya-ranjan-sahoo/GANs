{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "acgans.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTNOzP_DJVNN",
        "colab_type": "code",
        "outputId": "ff1a6262-9084-4313-dd62-89eceb2be412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6eec01ca50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEb_at0WT9Gd",
        "colab_type": "code",
        "outputId": "5989e499-38e0-4474-cf83-0d45cbfd48af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1m1Oi9GX9Jn",
        "colab_type": "code",
        "outputId": "97b5cb18-ada1-4228-990b-19bccaa4915e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/HLCV 19/Project/Project_HLCV_19/')\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracies_without_batch_normalization.png\n",
            "acgan\n",
            "checkpoint\n",
            "cnn_clf.pt\n",
            "code\n",
            "losses_without_batch_normalization.png\n",
            "model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5FtiZasirkK",
        "colab_type": "code",
        "outputId": "c3db6c72-4af8-4fdb-f5dc-f672e4efb500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def read_splits(folder,ext):\n",
        "  #folder=path+folder\n",
        "  files=os.listdir(folder)\n",
        "  fileids=[]\n",
        "  for f in files:\n",
        "    if ext in f:\n",
        "      fileids.append(folder+f)\n",
        "  return fileids\n",
        "\n",
        "# ft=read_splits('features_conv5_trained/','npy')\n",
        "ft=read_splits('code/evaluate/features_unconv4_trained/','npy')\n",
        "\n",
        "print(len(ft))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMWAkKx6Y4ZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=[]\n",
        "for f in ft:\n",
        "  train.append(np.load(f, allow_pickle = False))\n",
        "  \n",
        "train_features = train[0:454]  \n",
        "train_features=np.asarray(train_features)\n",
        "#train_features=np.reshape(train_features,(4540,-1))\n",
        "train_features = train_features.reshape(-1, *train_features.shape[-3:])\n",
        "train_features = np.swapaxes(train_features,1,3)\n",
        "\n",
        "test_features = train[454:]  \n",
        "test_features=np.asarray(test_features)\n",
        "#test_features=np.reshape(test_features,(570,-1))\n",
        "test_features = test_features.reshape(-1, *test_features.shape[-3:])\n",
        "test_features = np.swapaxes(test_features,1,3)\n",
        "\n",
        "#train_features=np.reshape(train_features,(len(ft),1024,-1))\n",
        "del train\n",
        "# testing with 1 feature\n",
        "#train_features= np.load(ft[0], allow_pickle = False)\n",
        "#train_features=np.reshape(train_features,(1024,-1))\n",
        "\n",
        "#from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "print(train_features.shape,test_features.shape)\n",
        "\n",
        "train_targets = np.load('code/evaluate/targets_conv5_trained/data_targets.npy', allow_pickle = False)\n",
        "test_targets = np.load('code/evaluate/targets_conv5_trained/test_targets.npy', allow_pickle = False)\n",
        "\n",
        "train_targets = np.squeeze(train_targets)\n",
        "test_targets = np.squeeze(test_targets)\n",
        "\n",
        "print(train_targets.shape,test_targets.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0Y1gvCFTQ9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxGd8VelUqHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db2QINhiWY3L",
        "colab_type": "code",
        "outputId": "8b3dfc63-0573-4050-8c04-fd972b99c5cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_classes = 2\n",
        "y = torch.eye(num_classes)\n",
        "\n",
        "test_targets = y[test_targets]\n",
        "print(test_targets.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([570, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EbCRzSZyMiE",
        "colab_type": "code",
        "outputId": "d2809ef4-ce24-4236-9cbe-4acf97c00bad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "train[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-242b40412ca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-6ETpcmmS6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del train_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HjIwmnqVRw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class _netG(nn.Module):\n",
        "#   def __init__(self, ngpu, nz):\n",
        "#       super(_netG, self).__init__()\n",
        "#       self.ngpu = ngpu\n",
        "#       self.nz = nz\n",
        "\n",
        "#       # first linear layer\n",
        "#       self.fc1 = nn.Linear(nz, 384)\n",
        "#       # Transposed Convolution 2\n",
        "#       self.tconv2 = nn.Sequential(\n",
        "#           nn.ConvTranspose2d(384, 192, 4, 1, 0, bias=False),\n",
        "#           nn.BatchNorm2d(192),\n",
        "#           nn.ReLU(True),\n",
        "#       )\n",
        "#       # Transposed Convolution 3\n",
        "#       self.tconv3 = nn.Sequential(\n",
        "#           nn.ConvTranspose2d(192, 96, 4, 2, 1, bias=False),\n",
        "#           nn.BatchNorm2d(96),\n",
        "#           nn.ReLU(True),\n",
        "#       )\n",
        "#       # Transposed Convolution 4\n",
        "#       self.tconv4 = nn.Sequential(\n",
        "#           nn.ConvTranspose2d(96, 48, 4, 2, 1, bias=False),\n",
        "#           nn.BatchNorm2d(48),\n",
        "#           nn.ReLU(True),\n",
        "#       )\n",
        "#       # Transposed Convolution 4\n",
        "#       self.tconv5 = nn.Sequential(\n",
        "#           nn.ConvTranspose2d(48, 3, 4, 2, 1, bias=False),\n",
        "#           nn.Tanh(),\n",
        "#       )\n",
        "\n",
        "#   def forward(self, input):\n",
        "#       if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
        "#           input = input.view(-1, self.nz)\n",
        "#           fc1 = nn.parallel.data_parallel(self.fc1, input, range(self.ngpu))\n",
        "#           fc1 = fc1.view(-1, 384, 1, 1)\n",
        "#           tconv2 = nn.parallel.data_parallel(self.tconv2, fc1, range(self.ngpu))\n",
        "#           tconv3 = nn.parallel.data_parallel(self.tconv3, tconv2, range(self.ngpu))\n",
        "#           tconv4 = nn.parallel.data_parallel(self.tconv4, tconv3, range(self.ngpu))\n",
        "#           tconv5 = nn.parallel.data_parallel(self.tconv5, tconv4, range(self.ngpu))\n",
        "#           output = tconv5\n",
        "#       else:\n",
        "#           input = input.view(-1, self.nz)\n",
        "#           fc1 = self.fc1(input)\n",
        "#           fc1 = fc1.view(-1, 384, 1, 1)\n",
        "#           tconv2 = self.tconv2(fc1)\n",
        "#           tconv3 = self.tconv3(tconv2)\n",
        "#           tconv4 = self.tconv4(tconv3)\n",
        "#           tconv5 = self.tconv5(tconv4)\n",
        "#           output = tconv5\n",
        "#       return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9WQfemRU-n6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class _netD(nn.Module):\n",
        "#     def __init__(self, ngpu, num_classes=10):\n",
        "#         super(_netD, self).__init__()\n",
        "#         self.ngpu = ngpu\n",
        "\n",
        "#         # Convolution 1\n",
        "#         self.conv1 = nn.Sequential(\n",
        "#             nn.Conv2d(1, 16, 3, 1, 1, bias=False),\n",
        "#             nn.LeakyReLU(0.2, inplace=True),\n",
        "# #             nn.Dropout(0.5, inplace=False),\n",
        "#         )\n",
        "#         # Convolution 2\n",
        "#         self.conv2 = nn.Sequential(\n",
        "#             nn.Conv2d(16, 32, 3, 1, 1, bias=False),\n",
        "#             nn.BatchNorm2d(32),\n",
        "#             nn.LeakyReLU(0.2, inplace=True),\n",
        "# #             nn.Dropout(0.5, inplace=False),\n",
        "#         )\n",
        "#         # Convolution 3\n",
        "#         self.conv3 = nn.Sequential(\n",
        "#             nn.Conv2d(32, 64, 3, 2, 1, bias=False),\n",
        "#             nn.BatchNorm2d(64),\n",
        "#             nn.LeakyReLU(0.2, inplace=True),\n",
        "# #             nn.Dropout(0.5, inplace=False),\n",
        "#         )\n",
        "#         # Convolution 4\n",
        "#         self.conv4 = nn.Sequential(\n",
        "#             nn.Conv2d(64, 128, 3, 1, 1, bias=False),\n",
        "#             nn.BatchNorm2d(128),\n",
        "#             nn.LeakyReLU(0.2, inplace=True),\n",
        "# #             nn.Dropout(0.5, inplace=False),\n",
        "#         )\n",
        "#        # Convolution 5\n",
        "#         self.conv5 = nn.Sequential(\n",
        "#             nn.Conv2d(128, 256, 3, 2, 1, bias=False),\n",
        "#             nn.BatchNorm2d(256),\n",
        "#             nn.LeakyReLU(0.2, inplace=True),\n",
        "# #             nn.Dropout(0.5, inplace=False),\n",
        "#         )\n",
        "#        # Convolution 6\n",
        "#         self.conv6 = nn.Sequential(\n",
        "#             nn.Conv2d(256, 512, 3, 1, 1, bias=False),\n",
        "#             nn.BatchNorm2d(512),\n",
        "#             nn.LeakyReLU(0.2, inplace=True),\n",
        "# #             nn.Dropout(0.5, inplace=False),\n",
        "# #         )\n",
        "#         # discriminator fc\n",
        "#         self.fc_dis = nn.Linear(4*4*512, 1)\n",
        "#         # aux-classifier fc\n",
        "#         self.fc_aux = nn.Linear(4*4*512, num_classes)\n",
        "#         # softmax and sigmoid\n",
        "#         self.softmax = nn.Softmax()\n",
        "#         self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
        "#             conv1 = nn.parallel.data_parallel(self.conv1, input, range(self.ngpu))\n",
        "#             conv2 = nn.parallel.data_parallel(self.conv2, conv1, range(self.ngpu))\n",
        "#             conv3 = nn.parallel.data_parallel(self.conv3, conv2, range(self.ngpu))\n",
        "#             conv4 = nn.parallel.data_parallel(self.conv4, conv3, range(self.ngpu))\n",
        "#             conv5 = nn.parallel.data_parallel(self.conv5, conv4, range(self.ngpu))\n",
        "#             conv6 = nn.parallel.data_parallel(self.conv6, conv5, range(self.ngpu))\n",
        "#             flat6 = conv4.view(-1, 4*4*512)\n",
        "#             fc_dis = nn.parallel.data_parallel(self.fc_dis, flat6, range(self.ngpu))\n",
        "#             fc_aux = nn.parallel.data_parallel(self.fc_aux, flat6, range(self.ngpu))\n",
        "#         else:\n",
        "#             conv1 = self.conv1(input)\n",
        "#             conv2 = self.conv2(conv1)\n",
        "#             conv3 = self.conv3(conv2)\n",
        "#             conv4 = self.conv4(conv3)\n",
        "#             conv5 = self.conv5(conv4)\n",
        "#             conv6 = self.conv6(conv5)\n",
        "#             flat6 = conv4.view(-1, 4*4*512)\n",
        "#             fc_dis = self.fc_dis(flat6)\n",
        "#             fc_aux = self.fc_aux(flat6)\n",
        "#         classes = self.softmax(fc_aux)\n",
        "#         realfake = self.sigmoid(fc_dis).view(-1, 1).squeeze(1)\n",
        "#         return realfake, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9SP3q9nlz6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # custom weights initialization called on netG and netD\n",
        "# def weights_init(m):\n",
        "#     classname = m.__class__.__name__\n",
        "#     if classname.find('Conv') != -1:\n",
        "#         m.weight.data.normal_(0.0, 0.02)\n",
        "#     elif classname.find('BatchNorm') != -1:\n",
        "#         m.weight.data.normal_(1.0, 0.02)\n",
        "#         m.bias.data.fill_(0)\n",
        "\n",
        "# compute the current classification accuracy\n",
        "def compute_acc(preds, labels):\n",
        "    correct = 0\n",
        "    preds_ = preds.data.max(1)[1]\n",
        "    correct = preds_.eq(labels.data).cpu().sum()\n",
        "    acc = float(correct) / float(len(labels.data)) * 100.0\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zE2RMaLlYUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# architecture from Xian et al 2018\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "class MLP_CRITIC(nn.Module):\n",
        "    def __init__(self, ngpu, num_classes=10): \n",
        "        super(MLP_CRITIC, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.fc1 = nn.Linear(resSize, ndh)\n",
        "        #self.fc2 = nn.Linear(ndh, ndh)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "        # discriminator fc\n",
        "        self.fc_dis = nn.Linear(ndh, 1)\n",
        "        # aux-classifier fc\n",
        "        self.fc_aux = nn.Linear(ndh, num_classes)\n",
        "        # softmax and sigmoid\n",
        "        self.softmax = nn.Softmax()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.lrelu(self.fc1(x))\n",
        "#         h = self.fc2(h)\n",
        "        fc_dis = self.fc_dis(h)\n",
        "        fc_aux = self.fc_aux(h)\n",
        "        classes = self.softmax(fc_aux)\n",
        "        realfake = self.sigmoid(fc_dis).view(-1, 1).squeeze(1)\n",
        "        return realfake, classes\n",
        "\n",
        "\n",
        "class MLP_G(nn.Module):\n",
        "    def __init__(self, ngpu,nz):\n",
        "        super(MLP_G, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.nz = nz\n",
        "        \n",
        "        self.fc1 = nn.Linear(nz, ngh)\n",
        "        self.fc2 = nn.Linear(ngh, resSize)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "        #self.prelu = nn.PReLU()\n",
        "        self.relu = nn.ReLU(True)\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, noise):\n",
        "        input = noise.view(-1, self.nz)\n",
        "        h = self.lrelu(self.fc1(input))\n",
        "        h = self.relu(self.fc2(h))\n",
        "        return h\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpECaE9c39XN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_labels = np.random.randint(0, num_classes, 1024)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQClewBrC9Ii",
        "colab_type": "code",
        "outputId": "42d3620c-4634-40dc-81c8-e929f4ebf8b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#--------------------------------\n",
        "# Device configuration\n",
        "#--------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: %s'%device)\n",
        "\n",
        "#--------------------------------\n",
        "# Hyper-parameters\n",
        "#--------------------------------\n",
        "num_classes = 2\n",
        "resSize = train_features.shape[1] #size of visual features\n",
        "# attSize = 1024 #size of semantic features\n",
        "nz = 312 #size of the latent z vector \n",
        "ngh = 4096 #size of the hidden units in generator\n",
        "ndh = 1024 #size of the hidden units in discriminator\n",
        "# hidden_size = [128, 512, 512, 512, 512, 512]\n",
        "num_epochs = 5\n",
        "batch_size = 200\n",
        "learning_rate = 2e-3\n",
        "learning_rate_decay = 0.95\n",
        "reg=0.001\n",
        "beta_1=0.5\n",
        "\n",
        "num_training= train_features.shape[0]\n",
        "num_validation = test_features.shape[0]\n",
        "# print(hidden_size)\n",
        "\n",
        "train_data = []\n",
        "val_data=[]\n",
        "for i in range(num_training):\n",
        "   train_data.append([train_features[i], train_targets[i]])\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for i in range(num_validation):\n",
        "   val_data.append([test_features[i], test_targets[i]])\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VNwkkJ6YtrKh",
        "colab": {}
      },
      "source": [
        "# ngpu=1\n",
        "# netG = MLP_G(ngpu, nz)\n",
        "# netG.apply(weights_init)\n",
        "# print(netG)\n",
        "\n",
        "# netD = MLP_CRITIC(ngpu, num_classes)\n",
        "# netD.apply(weights_init)\n",
        "\n",
        "# print(netD)\n",
        "\n",
        "# # loss functions\n",
        "# dis_criterion = nn.BCELoss()\n",
        "# aux_criterion = nn.NLLLoss()\n",
        "\n",
        "# # tensor placeholders\n",
        "# x = torch.FloatTensor(batch_size, 3, input_size, input_size)\n",
        "# noise = torch.FloatTensor(batch_size, nz, 1, 1)\n",
        "# eval_noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)\n",
        "# dis_label = torch.FloatTensor(batch_size)\n",
        "# aux_label = torch.LongTensor(batch_size)\n",
        "# real_label = 1\n",
        "# fake_label = 0\n",
        "\n",
        "# # if using cuda\n",
        "# if device=='cuda':\n",
        "#     netD.cuda()\n",
        "#     netG.cuda()\n",
        "#     dis_criterion.cuda()\n",
        "#     aux_criterion.cuda()\n",
        "#     x, dis_label, aux_label = x.cuda(), dis_label.cuda(), aux_label.cuda()\n",
        "#     noise, eval_noise = noise.cuda(), eval_noise.cuda()\n",
        "\n",
        "# # define variables\n",
        "# x = Variable(x)\n",
        "# noise = Variable(noise)\n",
        "# eval_noise = Variable(eval_noise)\n",
        "# dis_label = Variable(dis_label)\n",
        "# aux_label = Variable(aux_label)\n",
        "# # noise for evaluation\n",
        "# eval_noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
        "# eval_label = np.random.randint(0, num_classes, batch_size)\n",
        "# eval_onehot = np.zeros((batch_size, num_classes))\n",
        "# eval_onehot[np.arange(batch_size), eval_label] = 1\n",
        "# eval_noise_[np.arange(batch_size), :num_classes] = eval_onehot[np.arange(batch_size)]\n",
        "# eval_noise_ = (torch.from_numpy(eval_noise_))\n",
        "# eval_noise.data.copy_(eval_noise_.view(batch_size, nz, 1, 1))\n",
        "\n",
        "# # setup optimizer\n",
        "# optimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(beta_1, 0.999))\n",
        "# optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(beta_1, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5TzplME_9As",
        "colab_type": "code",
        "outputId": "9e003f5a-7e49-4714-846e-b72362fd7f3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "ngpu=1\n",
        "netG = MLP_G(ngpu, nz)\n",
        "netG.apply(weights_init)\n",
        "print(netG)\n",
        "\n",
        "netD = MLP_CRITIC(ngpu, num_classes)\n",
        "netD.apply(weights_init)\n",
        "\n",
        "print(netD)\n",
        "\n",
        "# loss functions\n",
        "dis_criterion = nn.BCELoss()\n",
        "aux_criterion = nn.NLLLoss()\n",
        "\n",
        "# tensor placeholders\n",
        "x = torch.FloatTensor(batch_size, resSize)\n",
        "noise = torch.FloatTensor(batch_size, nz)\n",
        "eval_noise = torch.FloatTensor(batch_size, nz).normal_(0, 1)\n",
        "dis_label = torch.FloatTensor(batch_size)\n",
        "print(dis_label.shape)\n",
        "aux_label = torch.LongTensor(batch_size)\n",
        "print(aux_label.shape)\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "# if using cuda\n",
        "if device=='cuda':\n",
        "    netD.cuda()\n",
        "    netG.cuda()\n",
        "    dis_criterion.cuda()\n",
        "    aux_criterion.cuda()\n",
        "    x, dis_label, aux_label = x.cuda(), dis_label.cuda(), aux_label.cuda()\n",
        "    noise, eval_noise = noise.cuda(), eval_noise.cuda()\n",
        "\n",
        "# define variables\n",
        "x = Variable(x)\n",
        "noise = Variable(noise)\n",
        "eval_noise = Variable(eval_noise)\n",
        "dis_label = Variable(dis_label)\n",
        "aux_label = Variable(aux_label)\n",
        "# noise for evaluation\n",
        "eval_noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
        "eval_label = np.random.randint(0, num_classes, batch_size)\n",
        "eval_onehot = np.zeros((batch_size, num_classes))\n",
        "eval_onehot[np.arange(batch_size), eval_label] = 1\n",
        "eval_noise_[np.arange(batch_size), :num_classes] = eval_onehot[np.arange(batch_size)]\n",
        "eval_noise_ = (torch.from_numpy(eval_noise_))\n",
        "eval_noise.data.copy_(eval_noise_.view(batch_size, nz))\n",
        "\n",
        "# setup optimizer\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(beta_1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(beta_1, 0.999))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP_G(\n",
            "  (fc1): Linear(in_features=312, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=230400, bias=True)\n",
            "  (lrelu): LeakyReLU(negative_slope=0.2, inplace)\n",
            "  (relu): ReLU(inplace)\n",
            ")\n",
            "MLP_CRITIC(\n",
            "  (fc1): Linear(in_features=230400, out_features=1024, bias=True)\n",
            "  (lrelu): LeakyReLU(negative_slope=0.2, inplace)\n",
            "  (fc_dis): Linear(in_features=1024, out_features=1, bias=True)\n",
            "  (fc_aux): Linear(in_features=1024, out_features=2, bias=True)\n",
            "  (softmax): Softmax()\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKpebTc88Ny6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# avg_loss_D = 0.0\n",
        "# avg_loss_G = 0.0\n",
        "# avg_loss_A = 0.0\n",
        "# for epoch in range(num_epochs):\n",
        "#     for i, data in enumerate(train_loader):\n",
        "#         ############################\n",
        "#         # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "#         ###########################\n",
        "#         # train with real\n",
        "#         with torch.no_grad():\n",
        "#           real_cpu, label = data\n",
        "#           batch_size = real_cpu.size(0)\n",
        "#           x.resize_(real_cpu.size()).copy_(real_cpu)\n",
        "#         dis_label.data.resize_(batch_size).fill_(real_label)\n",
        "#         aux_label.data.resize_(batch_size).copy_(labe0..\n",
        "#                                                  l)\n",
        "#         dis_output, aux_output = netD(x)\n",
        "\n",
        "#         dis_errD_real = dis_criterion(dis_output, dis_label)\n",
        "#         aux_errD_real = aux_criterion(aux_output, aux_label)\n",
        "#         errD_real = dis_errD_real + aux_errD_real\n",
        "#         errD_real.backward()\n",
        "#         D_x = dis_output.data.mean()\n",
        "\n",
        "#         # compute the current classification accuracy\n",
        "#         accuracy = compute_acc(aux_output, aux_label)\n",
        "\n",
        "#         # train with fake\n",
        "#         noise.data.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
        "#         label = np.random.randint(0, num_classes, batch_size)\n",
        "#         noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
        "#         class_onehot = np.zeros((batch_size, num_classes))\n",
        "#         class_onehot[np.arange(batch_size), label] = 1\n",
        "#         noise_[np.arange(batch_size), :num_classes] = class_onehot[np.arange(batch_size)]\n",
        "#         noise_ = (torch.from_numpy(noise_))\n",
        "#         noise.data.copy_(noise_.view(batch_size, nz, 1, 1))\n",
        "#         aux_label.data.resize_(batch_size).copy_(torch.from_numpy(label))\n",
        "\n",
        "#         fake = netG(noise)\n",
        "#         dis_label.data.fill_(fake_label)\n",
        "#         dis_output, aux_output = netD(fake.detach())\n",
        "#         dis_errD_fake = dis_criterion(dis_output, dis_label)\n",
        "#         aux_errD_fake = aux_criterion(aux_output, aux_label)\n",
        "#         errD_fake = dis_errD_fake + aux_errD_fake\n",
        "#         errD_fake.backward()\n",
        "#         D_G_z1 = dis_output.data.mean()\n",
        "#         errD = errD_real + errD_fake\n",
        "#         optimizerD.step()\n",
        "\n",
        "#         ############################\n",
        "#         # (2) Update G network: maximize log(D(G(z)))\n",
        "#         ###########################\n",
        "#         netG.zero_grad()\n",
        "#         dis_label.data.fill_(real_label)  # fake labels are real for generator cost\n",
        "#         dis_output, aux_output = netD(fake)\n",
        "#         dis_errG = dis_criterion(dis_output, dis_label)\n",
        "#         aux_errG = aux_criterion(aux_output, aux_label)\n",
        "#         errG = dis_errG + aux_errG\n",
        "#         errG.backward()\n",
        "#         D_G_z2 = dis_output.data.mean()\n",
        "#         optimizerG.step()\n",
        "\n",
        "#         # compute the average loss\n",
        "#         curr_iter = epoch * len(train_loader) + i\n",
        "#         all_loss_G = avg_loss_G * curr_iter\n",
        "#         all_loss_D = avg_loss_D * curr_iter\n",
        "#         all_loss_A = avg_loss_A * curr_iter\n",
        "#         all_loss_G += errG.item()\n",
        "#         all_loss_D += errD.item()\n",
        "#         all_loss_A += accuracy\n",
        "#         avg_loss_G = all_loss_G / (curr_iter + 1)\n",
        "#         avg_loss_D = all_loss_D / (curr_iter + 1)\n",
        "#         avg_loss_A = all_loss_A / (curr_iter + 1)\n",
        "\n",
        "#         print('[%d/%d][%d/%d] Loss_D: %.4f (%.4f) Loss_G: %.4f (%.4f) D(x): %.4f D(G(z)): %.4f / %.4f Acc: %.4f (%.4f)'\n",
        "#               % (epoch, num_epochs, i, len(train_loader),\n",
        "#                  errD.item(), avg_loss_D, errG.item(), avg_loss_G, D_x, D_G_z1, D_G_z2, accuracy, avg_loss_A))\n",
        "#         if i % 100 == 0:\n",
        "#             vutils.save_image(real_cpu, 'real_samples.png')\n",
        "#             print('Label for eval = {}'.format(eval_label))\n",
        "#             fake = netG(eval_noise)\n",
        "#             vutils.save_image(fake.data,'fake_samples_epoch_%03d.png' % ( epoch))\n",
        "        \n",
        "            \n",
        "\n",
        "#     # do checkpointing\n",
        "#     torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (opt.outf, epoch))\n",
        "#     torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (opt.outf, epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDWCL8VOp1mO",
        "colab_type": "code",
        "outputId": "8321c479-d2f2-4f33-c4b7-6385950a26bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "avg_loss_D = 0.0\n",
        "avg_loss_G = 0.0\n",
        "avg_loss_A = 0.0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(train_loader):\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        # train with real\n",
        "        with torch.no_grad():\n",
        "          real_x, label= data\n",
        "          print(real_x.size())\n",
        "          batch_size = real_x.size(0)\n",
        "          x.resize_(real_x.size()).copy_(real_x)\n",
        "        dis_label.data.resize_(batch_size).fill_(real_label)\n",
        "        aux_label.data.resize_(batch_size).copy_(label)\n",
        "        dis_output, aux_output = netD(x)\n",
        "        print(dis_output.size())\n",
        "\n",
        "        dis_errD_real = dis_criterion(dis_output, dis_label)\n",
        "        aux_errD_real = aux_criterion(aux_output, aux_label)\n",
        "        errD_real = dis_errD_real + aux_errD_real\n",
        "        errD_real.backward()\n",
        "        D_x = dis_output.data.mean()\n",
        "\n",
        "        # compute the current classification accuracy\n",
        "        accuracy = compute_acc(aux_output, aux_label)\n",
        "\n",
        "        # train with fake\n",
        "        noise.data.resize_(batch_size, nz).normal_(0, 1)\n",
        "        label = np.random.randint(0, num_classes, batch_size)\n",
        "        noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
        "        class_onehot = np.zeros((batch_size, num_classes))\n",
        "        class_onehot[np.arange(batch_size), label] = 1\n",
        "        noise_[np.arange(batch_size), :num_classes] = class_onehot[np.arange(batch_size)]\n",
        "        noise_ = (torch.from_numpy(noise_))\n",
        "        noise.data.copy_(noise_.view(batch_size, nz))\n",
        "        aux_label.data.resize_(batch_size).copy_(torch.from_numpy(label))\n",
        "\n",
        "        fake = netG(noise)\n",
        "        dis_label.data.fill_(fake_label)\n",
        "        dis_output, aux_output = netD(fake.detach())\n",
        "        dis_errD_fake = dis_criterion(dis_output, dis_label)\n",
        "        aux_errD_fake = aux_criterion(aux_output, aux_label)\n",
        "        errD_fake = dis_errD_fake + aux_errD_fake\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = dis_output.data.mean()\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        dis_label.data.fill_(real_label)  # fake labels are real for generator cost\n",
        "        dis_output, aux_output = netD(fake)\n",
        "        dis_errG = dis_criterion(dis_output, dis_label)\n",
        "        aux_errG = aux_criterion(aux_output, aux_label)\n",
        "        errG = dis_errG + aux_errG\n",
        "        errG.backward()\n",
        "        D_G_z2 = dis_output.data.mean()\n",
        "        optimizerG.step()\n",
        "\n",
        "        # compute the average loss\n",
        "        curr_iter = epoch * len(train_loader) + i\n",
        "        all_loss_G = avg_loss_G * curr_iter\n",
        "        all_loss_D = avg_loss_D * curr_iter\n",
        "        all_loss_A = avg_loss_A * curr_iter\n",
        "        all_loss_G += errG.item()\n",
        "        all_loss_D += errD.item()\n",
        "        all_loss_A += accuracy\n",
        "        avg_loss_G = all_loss_G / (curr_iter + 1)\n",
        "        avg_loss_D = all_loss_D / (curr_iter + 1)\n",
        "        avg_loss_A = all_loss_A / (curr_iter + 1)\n",
        "\n",
        "        print('[%d/%d][%d/%d] Loss_D: %.4f (%.4f) Loss_G: %.4f (%.4f) D(x): %.4f D(G(z)): %.4f / %.4f Acc: %.4f (%.4f)'\n",
        "              % (epoch, num_epochs, i, len(train_loader),\n",
        "                 errD.item(), avg_loss_D, errG.item(), avg_loss_G, D_x, D_G_z1, D_G_z2, accuracy, avg_loss_A))\n",
        "        if i % 100 == 0:\n",
        "            print('Label for eval = {}'.format(eval_label))\n",
        "            fake = netG(eval_noise)\n",
        "        \n",
        "            \n",
        "\n",
        "    # do checkpointing\n",
        "#     torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (opt.outf, epoch))\n",
        "#     torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (opt.outf, epoch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 230400])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICz2JvKwOCO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class _netD(nn.Module):\n",
        "    def __init__(self, ngpu, num_classes=2):\n",
        "        super(_netD, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "\n",
        "        # Convolution 1\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, 3, 1, 1, bias=False),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "#             nn.Dropout(0.5, inplace=False),\n",
        "        )\n",
        "        # Convolution 2\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, 3, 1, 1, bias=False),\n",
        "            #nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "#             nn.Dropout(0.5, inplace=False),\n",
        "        )\n",
        "\n",
        "        # discriminator fc\n",
        "        #self.fc_dis = nn.Linear(7*7*512, 1)\n",
        "        # aux-classifier fc\n",
        "        self.fc_aux1 = nn.Linear(7*7*512, 4*4*256)\n",
        "        \n",
        "        self.fc_aux2 = nn.Linear(4*4*256, num_classes)\n",
        "        \n",
        "        # softmax and sigmoid\n",
        "        self.softmax = nn.Softmax()\n",
        "        #self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input):\n",
        "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
        "            conv1 = nn.parallel.data_parallel(self.conv1, input, range(self.ngpu))\n",
        "            conv2 = nn.parallel.data_parallel(self.conv2, conv1, range(self.ngpu))\n",
        "            \n",
        "            flat6 = conv2.view(-1, 7*7*512)\n",
        "            #fc_dis = nn.parallel.data_parallel(self.fc_dis, flat6, range(self.ngpu))\n",
        "            fc_aux = nn.parallel.data_parallel(self.fc_aux, flat6, range(self.ngpu))\n",
        "        else:\n",
        "            conv1 = self.conv1(input)\n",
        "            conv2 = self.conv2(conv1)\n",
        "            \n",
        "            flat6 = conv2.view(-1, 7*7*512)\n",
        "            #fc_dis = self.fc_dis(flat6)\n",
        "            fc_aux1 = self.fc_aux1(flat6)\n",
        "            \n",
        "            fc_aux2 = self.fc_aux2(fc_aux1)\n",
        "            \n",
        "        classes = self.softmax(fc_aux2)\n",
        "        #realfake = self.sigmoid(fc_dis).view(-1, 1).squeeze(1)\n",
        "        return classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajBDg_YMJVNS",
        "colab_type": "code",
        "outputId": "2edc3dba-01b0-4bdc-9167-926ca58846cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "def weights_init(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        m.weight.data.normal_(0.0, 1e-3)\n",
        "        m.bias.data.fill_(0.)\n",
        "\n",
        "def update_lr(optimizer, lr):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: %s'%device)\n",
        "\n",
        "        \n",
        "model = _netD(1).to(device)\n",
        "# Q2.a - Initialize the model with correct batch norm layer\n",
        "\n",
        "model.apply(weights_init)\n",
        "# Print the model\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "_netD(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
            "  )\n",
            "  (fc_aux1): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  (fc_aux2): Linear(in_features=4096, out_features=2, bias=True)\n",
            "  (softmax): Softmax()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrdmYOz9JVNV",
        "colab_type": "code",
        "outputId": "7398975f-83d5-40b0-8f78-107b4b22cc8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#--------------------------------\n",
        "# Hyper-parameters\n",
        "#--------------------------------\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "num_epochs = 20\n",
        "batch_size = 200\n",
        "learning_rate = 2e-3\n",
        "learning_rate_decay = 0.95\n",
        "reg=0.001\n",
        "\n",
        "rotation_=5\n",
        "translation_=0.1\n",
        "hue_=0.05\n",
        "saturation_=0.05\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
        "# *****START OF MODIFIED CODE (SELF-ADDED)*****\n",
        "train_losses = []\n",
        "train_loss = 0\n",
        "val_losses = []\n",
        "val_loss = 0\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "early_stopping = 0\n",
        "batch_counter = 0\n",
        "# *****END OF MODIFIED CODE (SELF-ADDED)*****\n",
        "# Train the model\n",
        "lr = learning_rate\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader): #245*200 training samples ? 200=batch_size?\n",
        "        # Move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        #print(labels)\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # *****START OF MODIFIED CODE (SELF-ADDED)*****\n",
        "        batch_counter = batch_counter + 1\n",
        "        train_loss += loss.item()\n",
        "        # *****END OF MODIFIED CODE (SELF-ADDED)*****\n",
        "        if (i+1) % 10 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "    # *****START OF MODIFIED CODE (SELF-ADDED)*****\n",
        "    train_losses.append(train_loss/batch_counter)\n",
        "    batch_counter = 0\n",
        "    train_loss = 0\n",
        "    # *****END OF MODIFIED CODE (SELF-ADDED)*****\n",
        "    # Code to update the lr\n",
        "    lr *= learning_rate_decay\n",
        "    update_lr(optimizer, lr)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            print(outputs.shape)\n",
        "            print(labels.shape)\n",
        "            _, labels = torch.max(labels.data, 1)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            print(predicted.shape)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            # *****START OF MODIFIED CODE (SELF-ADDED)*****\n",
        "            batch_counter = batch_counter + 1\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "            # *****END OF MODIFIED CODE (SELF-ADDED)*****\n",
        "            \n",
        "        # *****START OF MODIFIED CODE (SELF-ADDED)*****\n",
        "        val_losses.append(val_loss/batch_counter)\n",
        "        batch_counter = 0\n",
        "        val_loss = 0\n",
        "        # *****END OF MODIFIED CODE (SELF-ADDED)*****\n",
        "\n",
        "        print('Validataion accuracy is: {} %'.format(100 * correct / total))\n",
        "        #################################################################################\n",
        "        # TODO: Q2.b Implement the early stopping mechanism to save the model which has #\n",
        "        # acheieved the best validation accuracy so-far.                                #\n",
        "        #################################################################################\n",
        "        best_model = None\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        val_acc = 100 * correct / total\n",
        "        if epoch == 0:\n",
        "          val_acc_max = 0\n",
        "        if val_acc > val_acc_max:\n",
        "            best_model = val_acc\n",
        "            print('Validation accuracy increased ({} --> {}).  Saving model ...'.format(val_acc_max, val_acc))\n",
        "            torch.save(model.state_dict(), 'model.ckpt')\n",
        "            val_acc_max = val_acc\n",
        "            early_stopping = epoch\n",
        "        else:\n",
        "            print(\"No improvement in validation accuracy\")\n",
        "        \n",
        "        val_accuracies.append(val_acc)\n",
        "        \n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        \n",
        "        # *****START OF MODIFIED CODE (SELF-ADDED)*****\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        print('Train accuracy is: {} %'.format(100 * correct / total))\n",
        "        train_accuracies.append(100 * correct / total)      \n",
        "        # *****END OF MODIFIED CODE (SELF-ADDED)*****\n",
        "\n",
        "    model.train()\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "model.eval()\n",
        "# *****START OF MODIFIED CODE (SELF-ADDED)*****\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "plt.figure().gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "plt.plot(train_losses, label='Training loss', marker='o')\n",
        "plt.plot(val_losses, label='Validation loss', marker='o')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.savefig('losses_without_batch_normalization.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure().gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "plt.plot(train_accuracies, label='Train Accuracies', marker='o')\n",
        "plt.plot(val_accuracies, label='Validation Accuracies', marker='o')\n",
        "plt.axvline(x = early_stopping, color='k', linestyle='--', label='Early Stopping')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylim((0, 100))\n",
        "plt.savefig('accuracies_without_batch_normalization.png')\n",
        "plt.show()\n",
        "# *****END OF MODIFIED CODE (SELF-ADDED)*****\n",
        "# *****START OF MODIFIED CODE (SELF-ADDED)*****\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# import json\n",
        "# path = '/content/gdrive/My Drive'\n",
        "# *****END OF MODIFIED CODE (SELF-ADDED)*****"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Step [10/23], Loss: 0.9583\n",
            "Epoch [1/20], Step [20/23], Loss: 0.8733\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200])\n",
            "torch.Size([170, 2])\n",
            "torch.Size([170, 2])\n",
            "torch.Size([170])\n",
            "Validataion accuracy is: 36.49122807017544 %\n",
            "Validation accuracy increased (0 --> 36.49122807017544).  Saving model ...\n",
            "Train accuracy is: 40.26431718061674 %\n",
            "Epoch [2/20], Step [10/23], Loss: 0.9233\n",
            "Epoch [2/20], Step [20/23], Loss: 0.7533\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200])\n",
            "torch.Size([170, 2])\n",
            "torch.Size([170, 2])\n",
            "torch.Size([170])\n",
            "Validataion accuracy is: 63.50877192982456 %\n",
            "Validation accuracy increased (36.49122807017544 --> 63.50877192982456).  Saving model ...\n",
            "Train accuracy is: 59.73568281938326 %\n",
            "Epoch [3/20], Step [10/23], Loss: 0.7433\n",
            "Epoch [3/20], Step [20/23], Loss: 0.6933\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200])\n",
            "torch.Size([170, 2])\n",
            "torch.Size([170, 2])\n",
            "torch.Size([170])\n",
            "Validataion accuracy is: 63.50877192982456 %\n",
            "No improvement in validation accuracy\n",
            "Train accuracy is: 59.73568281938326 %\n",
            "Epoch [4/20], Step [10/23], Loss: 0.9883\n",
            "Epoch [4/20], Step [20/23], Loss: 0.8783\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200])\n",
            "torch.Size([170, 2])\n",
            "torch.Size([170, 2])\n",
            "torch.Size([170])\n",
            "Validataion accuracy is: 36.49122807017544 %\n",
            "No improvement in validation accuracy\n",
            "Train accuracy is: 40.26431718061674 %\n",
            "Epoch [5/20], Step [10/23], Loss: 0.7533\n",
            "Epoch [5/20], Step [20/23], Loss: 0.8833\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200])\n",
            "torch.Size([170, 2])\n",
            "torch.Size([170, 2])\n",
            "torch.Size([170])\n",
            "Validataion accuracy is: 63.50877192982456 %\n",
            "No improvement in validation accuracy\n",
            "Train accuracy is: 59.73568281938326 %\n",
            "Epoch [6/20], Step [10/23], Loss: 0.6883\n",
            "Epoch [6/20], Step [20/23], Loss: 0.7283\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200, 2])\n",
            "torch.Size([200])\n",
            "torch.Size([170, 2])\n",
            "torch.Size([170, 2])\n",
            "torch.Size([170])\n",
            "Validataion accuracy is: 36.49122807017544 %\n",
            "No improvement in validation accuracy\n",
            "Train accuracy is: 40.26431718061674 %\n",
            "Epoch [7/20], Step [10/23], Loss: 0.9683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-bce9da87503d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# *****START OF MODIFIED CODE (SELF-ADDED)*****\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mbatch_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_counter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;31m# *****END OF MODIFIED CODE (SELF-ADDED)*****\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yIeMUIBJVNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pcV8hMZJVNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivkhg_-IJVNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv9fYnCmJVNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB2LpTseJVNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ3ZNYWuJVNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YydhS8EcJVNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztNoYB5nJVN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}